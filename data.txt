RAG Chat is a full-stack conversational AI application built with FastAPI, React, and Cerebras LLM (gpt-oss-120b with 65,536 token context window). It uses retrieval-augmented generation to combine a private FAISS knowledge base with a PostgreSQL pgvector database for multi-layered memory. The three memory layers are: document memory (FAISS chunks from data.txt), cross-conversation Q&A memory (pgvector semantic similarity), and same-conversation Q&A memory (within-thread context reuse).

The backend uses an intent classification system that routes every user query through five categories: general (open-ended questions), knowledge_base (questions answerable from the FAISS index), continuation (follow-up references using pronouns like "it", "that", "they"), profile (personal information updates or queries), and privacy (data access/deletion requests). Classification uses a two-stage approach: fast pre-heuristics (regex pattern matching for privacy signals, continuation pronouns, personal references) followed by LLM-based classification when heuristics are inconclusive. Each classification returns an intent label and confidence score (0.0 to 1.0).

The BehaviorPolicy engine (policy.py) analyzes context features from the conversation to make runtime decisions. It extracts features like is_greeting (detects "hi", "hello", "hey"), has_personal_reference, is_profile_statement, and conversation state. The policy outputs a PolicyDecision containing privacy_mode (whether to limit data exposure), greeting_name (user's name for personalization), skip_retrieval (whether to bypass knowledge base search), and disable_profile_detect (whether to skip profile detection). This prevents unnecessary computation ‚Äî greetings don't need FAISS retrieval, privacy queries need data audit responses.

The prompt orchestrator (prompt_orchestrator.py) builds messages for the LLM using a modular context frame system. It constructs: a system prompt that defines the assistant's personality, optional context frames for document chunks, cross-conversation Q&A, same-conversation Q&A, user profile data, and topic similarity scores. The orchestrator respects the policy decision ‚Äî it omits profile data during privacy mode, personalizes greetings with names, and skips retrieval frames when the policy says to.

The LLM module (llm/ package) is split into seven files: client.py (Cerebras API wrapper with completion function, model constants, and token limits), prompts.py (all template strings including SYSTEM_PROMPT, INTENT_PROMPT, five context frame templates, PROFILE_DETECT_PROMPT, and TITLE_PROMPT), classifier.py (intent classification with pre-heuristics), prompt_orchestrator.py (policy-aware message builder), generators.py (response generation: batch mode, streaming with Vercel AI SDK data stream protocol, and title generation), profile_detector.py (detects personal information in user messages using PERSONAL_SIGNALS pre-check then LLM JSON parsing), and __init__.py (barrel re-exports).

FAISS (Facebook AI Similarity Search) indexes document chunks from data.txt using the sentence-transformers all-MiniLM-L6-v2 model to generate 384-dimensional embeddings. Documents are chunked at 500 characters with 450-character overlap to maintain context boundaries. The FAISS index uses L2 distance and returns top-k nearest neighbors for knowledge_base queries. This enables semantic search ‚Äî finding contextually relevant information even when exact keywords do not match.

PostgreSQL 16 with the pgvector extension provides persistent storage for conversations, messages, Q&A pairs, user profile entries, and embedding-based similarity search. The schema includes tables for conversations (with UUID primary keys and auto-generated titles), messages (with role, content, and conversation foreign keys), qa_embeddings (storing question-answer pairs with 384-dim vectors for cross-conversation retrieval), and user_profile (key-value pairs with categories). Connection pooling uses psycopg2's SimpleConnectionPool (1 to 10 connections) with health-check wrappers. The DATABASE_URL environment variable configures the connection.

The streaming protocol follows the Vercel AI SDK data stream format. Text tokens are sent as '0:"token"\\n', metadata annotations as '8:[{...}]\\n', finish signals as 'e:{...}\\n', and done signals as 'd:{...}\\n'. The backend emits three streaming stage events before token generation begins: a "classified" stage (with intent and confidence), a "retrieved" stage (with retrieval_info when applicable), and a "generating" stage. After all tokens are streamed, a final annotation contains the complete metadata (intent, confidence, retrieval_info, query_tags).

The React frontend is built with React 18, Vite (dev server on port 5173 with proxy to backend port 8000), Tailwind CSS v3 with a custom dark theme, and Zustand for state management. The AI SDK useChat hook with streamProtocol: 'data' handles the streaming connection. The frontend includes an AI-native component layer (components/ai/) with: AIMessage (replaces basic Message with streaming phase indicators, status bar, intent badge, retrieval panel, debug panel, token meter), AIStatusBar (horizontal event timeline with clickable chips), AIIntentBadge (color-coded confidence indicator), AIRetrievalPanel (expandable drawer for RAG sources), AIDebugPanel (raw system internals), and AITokenMeter (context usage visualization).

The AI State Timeline renders above every assistant message as a horizontal chip bar showing the pipeline stages: [Classified: intent] ‚Üí [Retrieved: N docs] ‚Üí [Similar Q&A: N] ‚Üí [Topic: 0.61] ‚Üí [Profile injected] ‚Üí [Generating‚Ä¶/Complete]. Each chip represents a real backend operation that happened during processing. Clicking "Details" expands the AIRetrievalPanel showing the full retrieval breakdown across all three memory layers.

Debug Mode is an inspection feature toggleable from the sidebar or chat header. When enabled, every assistant message displays an AIDebugPanel showing the raw PolicyDecision JSON, full retrieval_info, query_tags, and classifier output. This makes invisible system decisions visible ‚Äî you can see exactly why the system chose knowledge_base over general, what documents were retrieved, whether profile data was injected, and what the confidence scores were.

The Command Palette (Ctrl+K or Cmd+K) provides quick keyboard-driven access to: creating new chats, switching to recent conversations, opening the profile manager, toggling debug mode, and other commands. It features fuzzy filtering as you type, arrow key navigation, and Enter to execute.

User profile management allows the system to learn personal preferences. When a user says "My name is Alex" or "I'm a Python developer", the profile detector identifies these as personal statements and stores them as key-value pairs in PostgreSQL (e.g., name: "Alex", occupation: "Python developer"). Profile data is then injected into the LLM context for future conversations, enabling personalized responses like greeting users by name.

The sidebar uses intelligent category icon detection based on conversation titles. Each conversation is analyzed against regex patterns to assign category icons: üß† Brain for ML/AI topics, üõ° Shield for privacy/security, üíæ Database for SQL/storage topics, üíª Code for programming, üìñ BookOpen for RAG/embedding topics, üåê Globe for web/deployment topics, and ‚ùì HelpCircle for general questions. This provides visual clustering of conversation topics at a glance.

Topic similarity tracking measures how related consecutive messages are within a conversation. The system computes cosine similarity between the current query's embedding and the previous turn's embedding, producing a score from 0.0 (completely different topic) to 1.0 (same topic). This score is used by the retrieval system to decide whether same-conversation Q&A context is relevant and by the UI to display topic continuity indicators.

The application architecture follows a pipeline pattern: user query ‚Üí BehaviorPolicy analysis ‚Üí intent classification (pre-heuristics then LLM) ‚Üí selective retrieval (FAISS + pgvector based on intent) ‚Üí prompt orchestration (policy-aware frame assembly) ‚Üí LLM generation (streaming with stage events) ‚Üí profile detection (async check for personal info) ‚Üí response delivery. Each stage is instrumented ‚Äî the frontend receives real-time notifications as each stage completes, enabling the AI State Timeline visualization.
